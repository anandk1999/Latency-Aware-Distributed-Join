┌─(...P/cs511-research-project)───(sarahgawde@Sarahs-MBP-2:s008)─┐
└─(16:16:43 on main ✹)──> docker-compose -f spark-cluster-compose.yaml exec main \
    spark-submit \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
    /opt/spark/scripts/kafka_join_optimizer.py
WARN[0000] /Users/sarahgawde/UIUC/Courses/511/RP/cs511-research-project/spark-cluster-compose.yaml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion 
OCI runtime exec failed: exec failed: unable to start container process: exec: " ": executable file not found in $PATH: unknown
zsh: command not found: spark-submit
zsh: command not found: --packages
┌─(...P/cs511-research-project)───(sarahgawde@Sarahs-MBP-2:s008)─┐
└─(16:17:27 on main ✹)──> docker-compose -f spark-cluster-compose.yaml exec main spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 /opt/spark/scripts/kafka_join_optimizer.py 
WARN[0000] /Users/sarahgawde/UIUC/Courses/511/RP/cs511-research-project/spark-cluster-compose.yaml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion 
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-cc55364f-94ff-43c0-bdbc-f6769cf2f8dc;1.0
        confs: [default]
        found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central
        found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central
        found org.apache.kafka#kafka-clients;3.4.1 in central
        found org.lz4#lz4-java;1.8.0 in central
        found org.xerial.snappy#snappy-java;1.1.10.3 in central
        found org.slf4j#slf4j-api;2.0.7 in central
        found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
        found org.apache.hadoop#hadoop-client-api;3.3.4 in central
        found commons-logging#commons-logging;1.1.3 in central
        found com.google.code.findbugs#jsr305;3.0.0 in central
        found org.apache.commons#commons-pool2;2.11.1 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.0/spark-sql-kafka-0-10_2.12-3.5.0.jar ...
        [SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0!spark-sql-kafka-0-10_2.12.jar (48ms)
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.0/spark-token-provider-kafka-0-10_2.12-3.5.0.jar ...
        [SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0!spark-token-provider-kafka-0-10_2.12.jar (26ms)
downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...
        [SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (160ms)
downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
        [SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (22ms)
downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
        [SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (28ms)
downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...
        [SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (695ms)
downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...
        [SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (39ms)
downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...
        [SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (71ms)
downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...
        [SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (23ms)
downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...
        [SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (475ms)
downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
        [SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (27ms)
:: resolution report :: resolve 2080ms :: artifacts dl 1623ms
        :: modules in use:
        com.google.code.findbugs#jsr305;3.0.0 from central in [default]
        commons-logging#commons-logging;1.1.3 from central in [default]
        org.apache.commons#commons-pool2;2.11.1 from central in [default]
        org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
        org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
        org.apache.kafka#kafka-clients;3.4.1 from central in [default]
        org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]
        org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]
        org.lz4#lz4-java;1.8.0 from central in [default]
        org.slf4j#slf4j-api;2.0.7 from central in [default]
        org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
        ---------------------------------------------------------------------
        |                  |            modules            ||   artifacts   |
        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
        ---------------------------------------------------------------------
        |      default     |   11  |   11  |   11  |   0   ||   11  |   11  |
        ---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-cc55364f-94ff-43c0-bdbc-f6769cf2f8dc
        confs: [default]
        11 artifacts copied, 0 already retrieved (56767kB/52ms)
24/11/26 22:18:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/11/26 22:18:36 INFO SparkContext: Running Spark version 3.4.1
24/11/26 22:18:36 INFO ResourceUtils: ==============================================================
24/11/26 22:18:36 INFO ResourceUtils: No custom resources configured for spark.driver.
24/11/26 22:18:36 INFO ResourceUtils: ==============================================================
24/11/26 22:18:36 INFO SparkContext: Submitted application: Kafka-Enabled Latency-Aware Distributed Join
24/11/26 22:18:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/11/26 22:18:36 INFO ResourceProfile: Limiting resource is cpu
24/11/26 22:18:36 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/11/26 22:18:36 INFO SecurityManager: Changing view acls to: root
24/11/26 22:18:36 INFO SecurityManager: Changing modify acls to: root
24/11/26 22:18:36 INFO SecurityManager: Changing view acls groups to: 
24/11/26 22:18:36 INFO SecurityManager: Changing modify acls groups to: 
24/11/26 22:18:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/11/26 22:18:36 INFO Utils: Successfully started service 'sparkDriver' on port 41627.
24/11/26 22:18:36 INFO SparkEnv: Registering MapOutputTracker
24/11/26 22:18:36 INFO SparkEnv: Registering BlockManagerMaster
24/11/26 22:18:36 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/11/26 22:18:36 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/11/26 22:18:36 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/11/26 22:18:36 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0e1cc81b-2c28-4f86-a453-a70b0f880e2d
24/11/26 22:18:36 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
24/11/26 22:18:36 INFO SparkEnv: Registering OutputCommitCoordinator
24/11/26 22:18:36 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/11/26 22:18:36 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/11/26 22:18:36 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://main:41627/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1732659516442
24/11/26 22:18:36 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://main:41627/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1732659516442
24/11/26 22:18:36 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://main:41627/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1732659516442
24/11/26 22:18:36 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://main:41627/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1732659516442
24/11/26 22:18:36 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://main:41627/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1732659516442
24/11/26 22:18:36 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://main:41627/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1732659516442
24/11/26 22:18:36 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://main:41627/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1732659516442
24/11/26 22:18:36 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://main:41627/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1732659516442
24/11/26 22:18:36 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://main:41627/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1732659516442
24/11/26 22:18:36 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://main:41627/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1732659516442
24/11/26 22:18:36 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://main:41627/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1732659516442
24/11/26 22:18:36 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://main:41627/files/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1732659516442
24/11/26 22:18:36 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-6d200df9-3cd9-40e1-b626-da293e0fec41/userFiles-a9d51a8f-8495-48ba-95b0-7ba7037c0b21/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
24/11/26 22:18:36 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://main:41627/files/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1732659516442
24/11/26 22:18:36 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-6d200df9-3cd9-40e1-b626-da293e0fec41/userFiles-a9d51a8f-8495-48ba-95b0-7ba7037c0b21/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
24/11/26 22:18:36 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://main:41627/files/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1732659516442
24/11/26 22:18:36 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-6d200df9-3cd9-40e1-b626-da293e0fec41/userFiles-a9d51a8f-8495-48ba-95b0-7ba7037c0b21/org.apache.kafka_kafka-clients-3.4.1.jar
24/11/26 22:18:36 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://main:41627/files/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1732659516442
24/11/26 22:18:36 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-6d200df9-3cd9-40e1-b626-da293e0fec41/userFiles-a9d51a8f-8495-48ba-95b0-7ba7037c0b21/com.google.code.findbugs_jsr305-3.0.0.jar
24/11/26 22:18:36 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://main:41627/files/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1732659516442
24/11/26 22:18:36 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-6d200df9-3cd9-40e1-b626-da293e0fec41/userFiles-a9d51a8f-8495-48ba-95b0-7ba7037c0b21/org.apache.commons_commons-pool2-2.11.1.jar
24/11/26 22:18:36 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://main:41627/files/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1732659516442
24/11/26 22:18:36 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-6d200df9-3cd9-40e1-b626-da293e0fec41/userFiles-a9d51a8f-8495-48ba-95b0-7ba7037c0b21/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/11/26 22:18:36 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://main:41627/files/org.lz4_lz4-java-1.8.0.jar with timestamp 1732659516442
24/11/26 22:18:36 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-6d200df9-3cd9-40e1-b626-da293e0fec41/userFiles-a9d51a8f-8495-48ba-95b0-7ba7037c0b21/org.lz4_lz4-java-1.8.0.jar
24/11/26 22:18:36 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://main:41627/files/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1732659516442
24/11/26 22:18:36 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-6d200df9-3cd9-40e1-b626-da293e0fec41/userFiles-a9d51a8f-8495-48ba-95b0-7ba7037c0b21/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/11/26 22:18:36 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://main:41627/files/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1732659516442
24/11/26 22:18:36 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-6d200df9-3cd9-40e1-b626-da293e0fec41/userFiles-a9d51a8f-8495-48ba-95b0-7ba7037c0b21/org.slf4j_slf4j-api-2.0.7.jar
24/11/26 22:18:36 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://main:41627/files/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1732659516442
24/11/26 22:18:36 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-6d200df9-3cd9-40e1-b626-da293e0fec41/userFiles-a9d51a8f-8495-48ba-95b0-7ba7037c0b21/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/11/26 22:18:36 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://main:41627/files/commons-logging_commons-logging-1.1.3.jar with timestamp 1732659516442
24/11/26 22:18:36 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-6d200df9-3cd9-40e1-b626-da293e0fec41/userFiles-a9d51a8f-8495-48ba-95b0-7ba7037c0b21/commons-logging_commons-logging-1.1.3.jar
24/11/26 22:18:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://main:7077...
24/11/26 22:18:37 INFO TransportClientFactory: Successfully created connection to main/192.168.10.2:7077 after 16 ms (0 ms spent in bootstraps)
24/11/26 22:18:37 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20241126221837-0000
24/11/26 22:18:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43653.
24/11/26 22:18:37 INFO NettyBlockTransferService: Server created on main:43653
24/11/26 22:18:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/11/26 22:18:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, main, 43653, None)
24/11/26 22:18:37 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241126221837-0000/0 on worker-20241126221447-192.168.10.3-42489 (192.168.10.3:42489) with 11 core(s)
24/11/26 22:18:37 INFO StandaloneSchedulerBackend: Granted executor ID app-20241126221837-0000/0 on hostPort 192.168.10.3:42489 with 11 core(s), 1024.0 MiB RAM
24/11/26 22:18:37 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241126221837-0000/1 on worker-20241126221447-192.168.10.2-34561 (192.168.10.2:34561) with 11 core(s)
24/11/26 22:18:37 INFO BlockManagerMasterEndpoint: Registering block manager main:43653 with 366.3 MiB RAM, BlockManagerId(driver, main, 43653, None)
24/11/26 22:18:37 INFO StandaloneSchedulerBackend: Granted executor ID app-20241126221837-0000/1 on hostPort 192.168.10.2:34561 with 11 core(s), 1024.0 MiB RAM
24/11/26 22:18:37 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241126221837-0000/2 on worker-20241126221447-192.168.10.4-44127 (192.168.10.4:44127) with 11 core(s)
24/11/26 22:18:37 INFO StandaloneSchedulerBackend: Granted executor ID app-20241126221837-0000/2 on hostPort 192.168.10.4:44127 with 11 core(s), 1024.0 MiB RAM
24/11/26 22:18:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, main, 43653, None)
24/11/26 22:18:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, main, 43653, None)
24/11/26 22:18:37 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241126221837-0000/0 is now RUNNING
24/11/26 22:18:37 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241126221837-0000/2 is now RUNNING
24/11/26 22:18:37 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241126221837-0000/1 is now RUNNING
24/11/26 22:18:37 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
24/11/26 22:18:38 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/11/26 22:18:38 INFO SharedState: Warehouse path is 'file:/spark-warehouse'.
24/11/26 22:18:39 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.10.4:54982) with ID 2,  ResourceProfileId 0
24/11/26 22:18:39 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.10.3:38800) with ID 0,  ResourceProfileId 0
24/11/26 22:18:39 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.10.2:33080) with ID 1,  ResourceProfileId 0
24/11/26 22:18:39 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.10.3:41305 with 366.3 MiB RAM, BlockManagerId(0, 192.168.10.3, 41305, None)
24/11/26 22:18:39 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.10.4:40553 with 366.3 MiB RAM, BlockManagerId(2, 192.168.10.4, 40553, None)
24/11/26 22:18:39 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.10.2:43999 with 366.3 MiB RAM, BlockManagerId(1, 192.168.10.2, 43999, None)
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/usr/lib/python3.9/threading.py", line 954, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.9/threading.py", line 892, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/spark/scripts/kafka_join_optimizer.py", line 21, in consume_latency
    consumer = KafkaConsumer(
  File "/usr/local/lib/python3.9/dist-packages/kafka/consumer/group.py", line 356, in __init__
    self._client = KafkaClient(metrics=self._metrics, **self.config)
  File "/usr/local/lib/python3.9/dist-packages/kafka/client_async.py", line 244, in __init__
    self.config['api_version'] = self.check_version(timeout=check_timeout)
  File "/usr/local/lib/python3.9/dist-packages/kafka/client_async.py", line 900, in check_version
Traceback (most recent call last):
  File "/opt/spark/scripts/kafka_join_optimizer.py", line 153, in <module>
    raise Errors.NoBrokersAvailable()
kafka.errors.NoBrokersAvailable: NoBrokersAvailable
    main()
  File "/opt/spark/scripts/kafka_join_optimizer.py", line 142, in main
    result_df = join_optimizer.distributed_join(left_df, right_df, "id")
  File "/opt/spark/scripts/kafka_join_optimizer.py", line 98, in distributed_join
    left_size = left_df._jdf.storageInfo().memSize()
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 330, in get_return_value
py4j.protocol.Py4JError: An error occurred while calling o47.storageInfo. Trace:
py4j.Py4JException: Method storageInfo([]) does not exist
        at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)
        at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)
        at py4j.Gateway.invoke(Gateway.java:274)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
        at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
        at java.lang.Thread.run(Thread.java:750)


24/11/26 22:18:40 INFO SparkContext: Invoking stop() from shutdown hook
24/11/26 22:18:40 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/11/26 22:18:40 INFO SparkUI: Stopped Spark web UI at http://main:4040
24/11/26 22:18:40 INFO StandaloneSchedulerBackend: Shutting down all executors
24/11/26 22:18:40 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
24/11/26 22:18:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/11/26 22:18:40 INFO MemoryStore: MemoryStore cleared
24/11/26 22:18:40 INFO BlockManager: BlockManager stopped
24/11/26 22:18:40 INFO BlockManagerMaster: BlockManagerMaster stopped
24/11/26 22:18:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/11/26 22:18:40 INFO SparkContext: Successfully stopped SparkContext
24/11/26 22:18:40 INFO ShutdownHookManager: Shutdown hook called
24/11/26 22:18:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-6d200df9-3cd9-40e1-b626-da293e0fec41/pyspark-e53a5669-eb43-4cd3-bf1b-4c710131b981
24/11/26 22:18:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-a9958be2-419b-4064-b0d6-ab704edc525d
24/11/26 22:18:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-6d200df9-3cd9-40e1-b626-da293e0fec41
┌─(~/UIUC/Courses/511/RP/cs511-research-project)─────────────────────────────────────────────────────────(sarahgawde@Sarahs-MBP-2:s008)─┐
┌─(~/UIUC/Courses/511/RP/cs511-research-project)─────────────────────────────────────────────────────────(sarahgawde@Sarahs-MBP-2:s008)─┐
└─(16:20:19 on main ✹)──> docker-compose -f spark-cluster-compose.yaml exec main spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 /opt/spark/scripts/kafka_join_optimizer.py
WARN[0000] /Users/sarahgawde/UIUC/Courses/511/RP/cs511-research-project/spark-cluster-compose.yaml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion 
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-912f8dc5-b17a-4fff-9b20-16d08f5f6131;1.0
        confs: [default]
        found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central
        found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central
        found org.apache.kafka#kafka-clients;3.4.1 in central
        found org.lz4#lz4-java;1.8.0 in central
        found org.xerial.snappy#snappy-java;1.1.10.3 in central
        found org.slf4j#slf4j-api;2.0.7 in central
        found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
        found org.apache.hadoop#hadoop-client-api;3.3.4 in central
        found commons-logging#commons-logging;1.1.3 in central
        found com.google.code.findbugs#jsr305;3.0.0 in central
        found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 230ms :: artifacts dl 7ms
        :: modules in use:
        com.google.code.findbugs#jsr305;3.0.0 from central in [default]
        commons-logging#commons-logging;1.1.3 from central in [default]
        org.apache.commons#commons-pool2;2.11.1 from central in [default]
        org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
        org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
        org.apache.kafka#kafka-clients;3.4.1 from central in [default]
        org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]
        org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]
        org.lz4#lz4-java;1.8.0 from central in [default]
        org.slf4j#slf4j-api;2.0.7 from central in [default]
        org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
        ---------------------------------------------------------------------
        |                  |            modules            ||   artifacts   |
        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
        ---------------------------------------------------------------------
        |      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
        ---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-912f8dc5-b17a-4fff-9b20-16d08f5f6131
        confs: [default]
        0 artifacts copied, 11 already retrieved (0kB/5ms)
24/11/26 22:24:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Traceback (most recent call last):
  File "/opt/spark/scripts/kafka_join_optimizer.py", line 2, in <module>
    from pyspark.util import SizeEstimator
ImportError: cannot import name 'SizeEstimator' from 'pyspark.util' (/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py)
24/11/26 22:24:06 INFO ShutdownHookManager: Shutdown hook called
24/11/26 22:24:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-fe1c0293-604a-4a6d-8b21-45564132e7ce
┌─(~/UIUC/Courses/511/RP/cs511-research-project)─────────────────────────────────────────────────────────(sarahgawde@Sarahs-MBP-2:s008)─┐
└─(16:38:15 on main ✹)──> docker-compose -f spark-cluster-compose.yaml exec main spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 /opt/spark/scripts/kafka_join_optimizer.py
WARN[0000] /Users/sarahgawde/UIUC/Courses/511/RP/cs511-research-project/spark-cluster-compose.yaml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion 
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-ec3516a7-8f7b-444f-8760-f1ddc8196278;1.0
        confs: [default]
        found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central
        found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central
        found org.apache.kafka#kafka-clients;3.4.1 in central
        found org.lz4#lz4-java;1.8.0 in central
        found org.xerial.snappy#snappy-java;1.1.10.3 in central
        found org.slf4j#slf4j-api;2.0.7 in central
        found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
        found org.apache.hadoop#hadoop-client-api;3.3.4 in central
        found commons-logging#commons-logging;1.1.3 in central
        found com.google.code.findbugs#jsr305;3.0.0 in central
        found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 248ms :: artifacts dl 6ms
        :: modules in use:
        com.google.code.findbugs#jsr305;3.0.0 from central in [default]
        commons-logging#commons-logging;1.1.3 from central in [default]
        org.apache.commons#commons-pool2;2.11.1 from central in [default]
        org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
        org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
        org.apache.kafka#kafka-clients;3.4.1 from central in [default]
        org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]
        org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]
        org.lz4#lz4-java;1.8.0 from central in [default]
        org.slf4j#slf4j-api;2.0.7 from central in [default]
        org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
        ---------------------------------------------------------------------
        |                  |            modules            ||   artifacts   |
        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
        ---------------------------------------------------------------------
        |      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
        ---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-ec3516a7-8f7b-444f-8760-f1ddc8196278
        confs: [default]
        0 artifacts copied, 11 already retrieved (0kB/7ms)
24/11/26 22:42:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/11/26 22:42:21 INFO SparkContext: Running Spark version 3.4.1
24/11/26 22:42:21 INFO ResourceUtils: ==============================================================
24/11/26 22:42:21 INFO ResourceUtils: No custom resources configured for spark.driver.
24/11/26 22:42:21 INFO ResourceUtils: ==============================================================
24/11/26 22:42:21 INFO SparkContext: Submitted application: Kafka-Enabled Latency-Aware Distributed Join
24/11/26 22:42:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/11/26 22:42:21 INFO ResourceProfile: Limiting resource is cpu
24/11/26 22:42:21 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/11/26 22:42:21 INFO SecurityManager: Changing view acls to: root
24/11/26 22:42:21 INFO SecurityManager: Changing modify acls to: root
24/11/26 22:42:21 INFO SecurityManager: Changing view acls groups to: 
24/11/26 22:42:21 INFO SecurityManager: Changing modify acls groups to: 
24/11/26 22:42:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/11/26 22:42:21 INFO Utils: Successfully started service 'sparkDriver' on port 44825.
24/11/26 22:42:21 INFO SparkEnv: Registering MapOutputTracker
24/11/26 22:42:21 INFO SparkEnv: Registering BlockManagerMaster
24/11/26 22:42:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/11/26 22:42:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/11/26 22:42:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/11/26 22:42:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6dcbe34a-d690-442d-8bd3-040649df4f2e
24/11/26 22:42:21 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
24/11/26 22:42:21 INFO SparkEnv: Registering OutputCommitCoordinator
24/11/26 22:42:21 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/11/26 22:42:21 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/11/26 22:42:21 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://main:44825/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1732660941193
24/11/26 22:42:21 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://main:44825/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1732660941193
24/11/26 22:42:21 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://main:44825/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1732660941193
24/11/26 22:42:21 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://main:44825/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1732660941193
24/11/26 22:42:21 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://main:44825/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1732660941193
24/11/26 22:42:21 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://main:44825/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1732660941193
24/11/26 22:42:21 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://main:44825/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1732660941193
24/11/26 22:42:21 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://main:44825/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1732660941193
24/11/26 22:42:21 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://main:44825/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1732660941193
24/11/26 22:42:21 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://main:44825/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1732660941193
24/11/26 22:42:21 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://main:44825/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1732660941193
24/11/26 22:42:21 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://main:44825/files/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1732660941193
24/11/26 22:42:21 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-02b7dfe7-6412-4443-acb1-220561591d31/userFiles-acbd0e67-84b2-454b-a72e-95ff36021f26/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
24/11/26 22:42:21 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://main:44825/files/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1732660941193
24/11/26 22:42:21 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-02b7dfe7-6412-4443-acb1-220561591d31/userFiles-acbd0e67-84b2-454b-a72e-95ff36021f26/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
24/11/26 22:42:21 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://main:44825/files/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1732660941193
24/11/26 22:42:21 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-02b7dfe7-6412-4443-acb1-220561591d31/userFiles-acbd0e67-84b2-454b-a72e-95ff36021f26/org.apache.kafka_kafka-clients-3.4.1.jar
24/11/26 22:42:21 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://main:44825/files/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1732660941193
24/11/26 22:42:21 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-02b7dfe7-6412-4443-acb1-220561591d31/userFiles-acbd0e67-84b2-454b-a72e-95ff36021f26/com.google.code.findbugs_jsr305-3.0.0.jar
24/11/26 22:42:21 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://main:44825/files/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1732660941193
24/11/26 22:42:21 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-02b7dfe7-6412-4443-acb1-220561591d31/userFiles-acbd0e67-84b2-454b-a72e-95ff36021f26/org.apache.commons_commons-pool2-2.11.1.jar
24/11/26 22:42:21 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://main:44825/files/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1732660941193
24/11/26 22:42:21 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-02b7dfe7-6412-4443-acb1-220561591d31/userFiles-acbd0e67-84b2-454b-a72e-95ff36021f26/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/11/26 22:42:21 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://main:44825/files/org.lz4_lz4-java-1.8.0.jar with timestamp 1732660941193
24/11/26 22:42:21 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-02b7dfe7-6412-4443-acb1-220561591d31/userFiles-acbd0e67-84b2-454b-a72e-95ff36021f26/org.lz4_lz4-java-1.8.0.jar
24/11/26 22:42:21 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://main:44825/files/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1732660941193
24/11/26 22:42:21 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-02b7dfe7-6412-4443-acb1-220561591d31/userFiles-acbd0e67-84b2-454b-a72e-95ff36021f26/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/11/26 22:42:21 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://main:44825/files/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1732660941193
24/11/26 22:42:21 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-02b7dfe7-6412-4443-acb1-220561591d31/userFiles-acbd0e67-84b2-454b-a72e-95ff36021f26/org.slf4j_slf4j-api-2.0.7.jar
24/11/26 22:42:21 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://main:44825/files/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1732660941193
24/11/26 22:42:21 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-02b7dfe7-6412-4443-acb1-220561591d31/userFiles-acbd0e67-84b2-454b-a72e-95ff36021f26/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/11/26 22:42:21 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://main:44825/files/commons-logging_commons-logging-1.1.3.jar with timestamp 1732660941193
24/11/26 22:42:21 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-02b7dfe7-6412-4443-acb1-220561591d31/userFiles-acbd0e67-84b2-454b-a72e-95ff36021f26/commons-logging_commons-logging-1.1.3.jar
24/11/26 22:42:21 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://main:7077...
24/11/26 22:42:21 INFO TransportClientFactory: Successfully created connection to main/192.168.10.2:7077 after 13 ms (0 ms spent in bootstraps)
24/11/26 22:42:21 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20241126224221-0001
24/11/26 22:42:21 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241126224221-0001/0 on worker-20241126221447-192.168.10.3-42489 (192.168.10.3:42489) with 11 core(s)
24/11/26 22:42:21 INFO StandaloneSchedulerBackend: Granted executor ID app-20241126224221-0001/0 on hostPort 192.168.10.3:42489 with 11 core(s), 1024.0 MiB RAM
24/11/26 22:42:21 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241126224221-0001/1 on worker-20241126221447-192.168.10.2-34561 (192.168.10.2:34561) with 11 core(s)
24/11/26 22:42:21 INFO StandaloneSchedulerBackend: Granted executor ID app-20241126224221-0001/1 on hostPort 192.168.10.2:34561 with 11 core(s), 1024.0 MiB RAM
24/11/26 22:42:21 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241126224221-0001/2 on worker-20241126221447-192.168.10.4-44127 (192.168.10.4:44127) with 11 core(s)
24/11/26 22:42:21 INFO StandaloneSchedulerBackend: Granted executor ID app-20241126224221-0001/2 on hostPort 192.168.10.4:44127 with 11 core(s), 1024.0 MiB RAM
24/11/26 22:42:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37713.
24/11/26 22:42:21 INFO NettyBlockTransferService: Server created on main:37713
24/11/26 22:42:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/11/26 22:42:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, main, 37713, None)
24/11/26 22:42:21 INFO BlockManagerMasterEndpoint: Registering block manager main:37713 with 366.3 MiB RAM, BlockManagerId(driver, main, 37713, None)
24/11/26 22:42:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, main, 37713, None)
24/11/26 22:42:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, main, 37713, None)
24/11/26 22:42:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241126224221-0001/0 is now RUNNING
24/11/26 22:42:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241126224221-0001/1 is now RUNNING
24/11/26 22:42:21 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241126224221-0001/2 is now RUNNING
24/11/26 22:42:22 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
24/11/26 22:42:22 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/11/26 22:42:22 INFO SharedState: Warehouse path is 'file:/spark-warehouse'.
24/11/26 22:42:23 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.10.4:39716) with ID 2,  ResourceProfileId 0
24/11/26 22:42:23 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.10.2:57526) with ID 1,  ResourceProfileId 0
24/11/26 22:42:23 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.10.3:43346) with ID 0,  ResourceProfileId 0
24/11/26 22:42:23 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.10.4:43479 with 366.3 MiB RAM, BlockManagerId(2, 192.168.10.4, 43479, None)
24/11/26 22:42:23 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.10.3:43619 with 366.3 MiB RAM, BlockManagerId(0, 192.168.10.3, 43619, None)
24/11/26 22:42:23 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.10.2:34809 with 366.3 MiB RAM, BlockManagerId(1, 192.168.10.2, 34809, None)
24/11/26 22:42:25 INFO CodeGenerator: Code generated in 115.474167 ms
24/11/26 22:42:25 INFO SparkContext: Starting job: sumApprox at NativeMethodAccessorImpl.java:0
24/11/26 22:42:25 INFO DAGScheduler: Got job 0 (sumApprox at NativeMethodAccessorImpl.java:0) with 2 output partitions
24/11/26 22:42:25 INFO DAGScheduler: Final stage: ResultStage 0 (sumApprox at NativeMethodAccessorImpl.java:0)
24/11/26 22:42:25 INFO DAGScheduler: Parents of final stage: List()
24/11/26 22:42:25 INFO DAGScheduler: Missing parents: List()
24/11/26 22:42:25 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[15] at mapPartitions at SerDeUtil.scala:117), which has no missing parents
24/11/26 22:42:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 22.9 KiB, free 366.3 MiB)
24/11/26 22:42:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 366.3 MiB)
24/11/26 22:42:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on main:37713 (size: 9.2 KiB, free: 366.3 MiB)
24/11/26 22:42:25 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
24/11/26 22:42:25 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[15] at mapPartitions at SerDeUtil.scala:117) (first 15 tasks are for partitions Vector(0, 1))
24/11/26 22:42:25 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0
24/11/26 22:42:25 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.10.2, executor 1, partition 0, PROCESS_LOCAL, 7387 bytes) 
24/11/26 22:42:25 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (192.168.10.4, executor 2, partition 1, PROCESS_LOCAL, 7427 bytes) 
24/11/26 22:42:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.10.2:34809 (size: 9.2 KiB, free: 366.3 MiB)
24/11/26 22:42:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.10.4:43479 (size: 9.2 KiB, free: 366.3 MiB)
24/11/26 22:42:26 INFO SparkContext: Job finished: sumApprox at NativeMethodAccessorImpl.java:0, took 1.006827084 s
24/11/26 22:42:26 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1099 ms on 192.168.10.2 (executor 1) (1/2)
24/11/26 22:42:26 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1091 ms on 192.168.10.4 (executor 2) (2/2)
24/11/26 22:42:26 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/11/26 22:42:26 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 37355
24/11/26 22:42:26 INFO DAGScheduler: ResultStage 0 (sumApprox at NativeMethodAccessorImpl.java:0) finished in 1.205 s
24/11/26 22:42:26 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/11/26 22:42:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
{'approximate_rows': 3, 'estimated_row_size_bytes': 116, 'estimated_total_size_bytes': 348, 'estimated_total_size_mb': 0.000331878662109375}
24/11/26 22:42:26 INFO CodeGenerator: Code generated in 8.441208 ms
24/11/26 22:42:26 INFO DAGScheduler: Registering RDD 18 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
24/11/26 22:42:26 INFO DAGScheduler: Got map stage job 1 (count at NativeMethodAccessorImpl.java:0) with 33 output partitions
24/11/26 22:42:26 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (count at NativeMethodAccessorImpl.java:0)
24/11/26 22:42:26 INFO DAGScheduler: Parents of final stage: List()
24/11/26 22:42:26 INFO DAGScheduler: Missing parents: List()
24/11/26 22:42:26 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[18] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
24/11/26 22:42:26 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.1 KiB, free 366.3 MiB)
24/11/26 22:42:26 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.5 KiB, free 366.2 MiB)
24/11/26 22:42:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on main:37713 (size: 8.5 KiB, free: 366.3 MiB)
24/11/26 22:42:26 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
24/11/26 22:42:26 INFO DAGScheduler: Submitting 33 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[18] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
24/11/26 22:42:26 INFO TaskSchedulerImpl: Adding task set 1.0 with 33 tasks resource profile 0
24/11/26 22:42:26 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (192.168.10.2, executor 1, partition 0, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (192.168.10.4, executor 2, partition 1, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 4) (192.168.10.3, executor 0, partition 2, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 5) (192.168.10.2, executor 1, partition 3, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 6) (192.168.10.4, executor 2, partition 4, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 7) (192.168.10.3, executor 0, partition 5, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 8) (192.168.10.2, executor 1, partition 6, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 9) (192.168.10.4, executor 2, partition 7, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 10) (192.168.10.3, executor 0, partition 8, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 11) (192.168.10.2, executor 1, partition 9, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 12) (192.168.10.4, executor 2, partition 10, PROCESS_LOCAL, 7374 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 13) (192.168.10.3, executor 0, partition 11, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 12.0 in stage 1.0 (TID 14) (192.168.10.2, executor 1, partition 12, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 13.0 in stage 1.0 (TID 15) (192.168.10.4, executor 2, partition 13, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 14.0 in stage 1.0 (TID 16) (192.168.10.3, executor 0, partition 14, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 15.0 in stage 1.0 (TID 17) (192.168.10.2, executor 1, partition 15, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 16.0 in stage 1.0 (TID 18) (192.168.10.4, executor 2, partition 16, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 17.0 in stage 1.0 (TID 19) (192.168.10.3, executor 0, partition 17, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 18.0 in stage 1.0 (TID 20) (192.168.10.2, executor 1, partition 18, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 19.0 in stage 1.0 (TID 21) (192.168.10.4, executor 2, partition 19, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 20.0 in stage 1.0 (TID 22) (192.168.10.3, executor 0, partition 20, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 21.0 in stage 1.0 (TID 23) (192.168.10.2, executor 1, partition 21, PROCESS_LOCAL, 7377 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 22.0 in stage 1.0 (TID 24) (192.168.10.4, executor 2, partition 22, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 23.0 in stage 1.0 (TID 25) (192.168.10.3, executor 0, partition 23, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 24.0 in stage 1.0 (TID 26) (192.168.10.2, executor 1, partition 24, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 25.0 in stage 1.0 (TID 27) (192.168.10.4, executor 2, partition 25, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 26.0 in stage 1.0 (TID 28) (192.168.10.3, executor 0, partition 26, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 27.0 in stage 1.0 (TID 29) (192.168.10.2, executor 1, partition 27, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 28.0 in stage 1.0 (TID 30) (192.168.10.4, executor 2, partition 28, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 29.0 in stage 1.0 (TID 31) (192.168.10.3, executor 0, partition 29, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 30.0 in stage 1.0 (TID 32) (192.168.10.2, executor 1, partition 30, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 31.0 in stage 1.0 (TID 33) (192.168.10.4, executor 2, partition 31, PROCESS_LOCAL, 7336 bytes) 
24/11/26 22:42:26 INFO TaskSetManager: Starting task 32.0 in stage 1.0 (TID 34) (192.168.10.3, executor 0, partition 32, PROCESS_LOCAL, 7379 bytes) 
24/11/26 22:42:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.10.2:34809 (size: 8.5 KiB, free: 366.3 MiB)
24/11/26 22:42:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.10.4:43479 (size: 8.5 KiB, free: 366.3 MiB)
24/11/26 22:42:26 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.10.3:43619 (size: 8.5 KiB, free: 366.3 MiB)
24/11/26 22:42:27 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 12) in 655 ms on 192.168.10.4 (executor 2) (1/33)
24/11/26 22:42:27 INFO TaskSetManager: Finished task 16.0 in stage 1.0 (TID 18) in 654 ms on 192.168.10.4 (executor 2) (2/33)
24/11/26 22:42:27 INFO TaskSetManager: Finished task 13.0 in stage 1.0 (TID 15) in 658 ms on 192.168.10.4 (executor 2) (3/33)
24/11/26 22:42:27 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 661 ms on 192.168.10.4 (executor 2) (4/33)
24/11/26 22:42:27 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 6) in 662 ms on 192.168.10.4 (executor 2) (5/33)
24/11/26 22:42:27 INFO TaskSetManager: Finished task 22.0 in stage 1.0 (TID 24) in 671 ms on 192.168.10.4 (executor 2) (6/33)
24/11/26 22:42:27 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 9) in 675 ms on 192.168.10.4 (executor 2) (7/33)
24/11/26 22:42:27 INFO TaskSetManager: Finished task 19.0 in stage 1.0 (TID 21) in 679 ms on 192.168.10.4 (executor 2) (8/33)
24/11/26 22:42:27 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 5) in 684 ms on 192.168.10.2 (executor 1) (9/33)
24/11/26 22:42:27 INFO TaskSetManager: Finished task 27.0 in stage 1.0 (TID 29) in 679 ms on 192.168.10.2 (executor 1) (10/33)
24/11/26 22:42:27 INFO TaskSetManager: Finished task 28.0 in stage 1.0 (TID 30) in 680 ms on 192.168.10.4 (executor 2) (11/33)
24/11/26 22:42:27 INFO TaskSetManager: Finished task 25.0 in stage 1.0 (TID 27) in 682 ms on 192.168.10.4 (executor 2) (12/33)
24/11/26 22:42:27 INFO TaskSetManager: Finished task 21.0 in stage 1.0 (TID 23) in 684 ms on 192.168.10.2 (executor 1) (13/33)
24/11/26 22:42:27 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 690 ms on 192.168.10.2 (executor 1) (14/33)
24/11/26 22:42:27 INFO TaskSetManager: Finished task 31.0 in stage 1.0 (TID 33) in 683 ms on 192.168.10.4 (executor 2) (15/33)
24/11/26 22:42:27 INFO TaskSetManager: Finished task 30.0 in stage 1.0 (TID 32) in 684 ms on 192.168.10.2 (executor 1) (16/33)
24/11/26 22:42:27 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 8) in 690 ms on 192.168.10.2 (executor 1) (17/33)
24/11/26 22:42:27 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 11) in 689 ms on 192.168.10.2 (executor 1) (18/33)
24/11/26 22:42:27 INFO TaskSetManager: Finished task 18.0 in stage 1.0 (TID 20) in 687 ms on 192.168.10.2 (executor 1) (19/33)
24/11/26 22:42:27 INFO TaskSetManager: Finished task 12.0 in stage 1.0 (TID 14) in 690 ms on 192.168.10.2 (executor 1) (20/33)
24/11/26 22:42:27 INFO TaskSetManager: Finished task 24.0 in stage 1.0 (TID 26) in 687 ms on 192.168.10.2 (executor 1) (21/33)
24/11/26 22:42:27 INFO TaskSetManager: Finished task 15.0 in stage 1.0 (TID 17) in 689 ms on 192.168.10.2 (executor 1) (22/33)
24/11/26 22:42:28 INFO TaskSetManager: Finished task 23.0 in stage 1.0 (TID 25) in 1499 ms on 192.168.10.3 (executor 0) (23/33)
24/11/26 22:42:28 INFO TaskSetManager: Finished task 29.0 in stage 1.0 (TID 31) in 1498 ms on 192.168.10.3 (executor 0) (24/33)
24/11/26 22:42:28 INFO TaskSetManager: Finished task 32.0 in stage 1.0 (TID 34) in 1497 ms on 192.168.10.3 (executor 0) (25/33)
24/11/26 22:42:28 INFO TaskSetManager: Finished task 20.0 in stage 1.0 (TID 22) in 1501 ms on 192.168.10.3 (executor 0) (26/33)
24/11/26 22:42:28 INFO TaskSetManager: Finished task 26.0 in stage 1.0 (TID 28) in 1499 ms on 192.168.10.3 (executor 0) (27/33)
24/11/26 22:42:28 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 10) in 1504 ms on 192.168.10.3 (executor 0) (28/33)
24/11/26 22:42:28 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 4) in 1507 ms on 192.168.10.3 (executor 0) (29/33)
24/11/26 22:42:28 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 13) in 1506 ms on 192.168.10.3 (executor 0) (30/33)
24/11/26 22:42:28 INFO TaskSetManager: Finished task 17.0 in stage 1.0 (TID 19) in 1505 ms on 192.168.10.3 (executor 0) (31/33)
24/11/26 22:42:28 INFO TaskSetManager: Finished task 14.0 in stage 1.0 (TID 16) in 1506 ms on 192.168.10.3 (executor 0) (32/33)
24/11/26 22:42:28 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 7) in 1509 ms on 192.168.10.3 (executor 0) (33/33)
24/11/26 22:42:28 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/11/26 22:42:28 INFO DAGScheduler: ShuffleMapStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 1.526 s
24/11/26 22:42:28 INFO DAGScheduler: looking for newly runnable stages
24/11/26 22:42:28 INFO DAGScheduler: running: Set()
24/11/26 22:42:28 INFO DAGScheduler: waiting: Set()
24/11/26 22:42:28 INFO DAGScheduler: failed: Set()
24/11/26 22:42:28 INFO CodeGenerator: Code generated in 7.527 ms
24/11/26 22:42:28 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
24/11/26 22:42:28 INFO DAGScheduler: Got job 2 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/11/26 22:42:28 INFO DAGScheduler: Final stage: ResultStage 3 (count at NativeMethodAccessorImpl.java:0)
24/11/26 22:42:28 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
24/11/26 22:42:28 INFO DAGScheduler: Missing parents: List()
24/11/26 22:42:28 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[21] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
24/11/26 22:42:28 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 12.1 KiB, free 366.2 MiB)
24/11/26 22:42:28 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 366.2 MiB)
24/11/26 22:42:28 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on main:37713 (size: 5.8 KiB, free: 366.3 MiB)
24/11/26 22:42:28 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535
24/11/26 22:42:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[21] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/11/26 22:42:28 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
24/11/26 22:42:28 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 35) (192.168.10.4, executor 2, partition 0, NODE_LOCAL, 7367 bytes) 
24/11/26 22:42:28 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.10.4:43479 (size: 5.8 KiB, free: 366.3 MiB)
24/11/26 22:42:28 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.10.4:39716
24/11/26 22:42:28 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 35) in 185 ms on 192.168.10.4 (executor 2) (1/1)
24/11/26 22:42:28 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
24/11/26 22:42:28 INFO DAGScheduler: ResultStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 0.194 s
24/11/26 22:42:28 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/11/26 22:42:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
24/11/26 22:42:28 INFO DAGScheduler: Job 2 finished: count at NativeMethodAccessorImpl.java:0, took 0.208246 s
Traceback (most recent call last):
  File "/opt/spark/scripts/kafka_join_optimizer.py", line 300, in <module>
    main()
  File "/opt/spark/scripts/kafka_join_optimizer.py", line 277, in main
    result_df = join_optimizer.distributed_join(left_df, right_df, "id")
  File "/opt/spark/scripts/kafka_join_optimizer.py", line 241, in distributed_join
    strategy = self.choose_join_strategy(left_size_details, right_size)
  File "/opt/spark/scripts/kafka_join_optimizer.py", line 221, in choose_join_strategy
    smaller_size = min(left_size, right_size)
TypeError: '<' not supported between instances of 'int' and 'dict'
24/11/26 22:42:28 INFO SparkContext: Invoking stop() from shutdown hook
24/11/26 22:42:28 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/11/26 22:42:28 INFO SparkUI: Stopped Spark web UI at http://main:4040
24/11/26 22:42:28 INFO StandaloneSchedulerBackend: Shutting down all executors
24/11/26 22:42:28 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
24/11/26 22:42:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/11/26 22:42:28 INFO MemoryStore: MemoryStore cleared
24/11/26 22:42:28 INFO BlockManager: BlockManager stopped
24/11/26 22:42:28 INFO BlockManagerMaster: BlockManagerMaster stopped
24/11/26 22:42:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/11/26 22:42:28 INFO SparkContext: Successfully stopped SparkContext
24/11/26 22:42:28 INFO ShutdownHookManager: Shutdown hook called
24/11/26 22:42:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-02b7dfe7-6412-4443-acb1-220561591d31/pyspark-914fc44c-15c3-4618-8fd7-4f92c9ba007b
24/11/26 22:42:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-20393067-5d9b-4d7d-bd87-ca2d62334328
24/11/26 22:42:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-02b7dfe7-6412-4443-acb1-220561591d31
┌─(~/UIUC/Courses/511/RP/cs511-research-project)─────────────────────────────────────────────────────────(sarahgawde@Sarahs-MBP-2:s008)─┐
└─(16:42:28 on main ✹ ✭)──>                                                                                           1 ↵ ──(Tue,Nov26)─┘